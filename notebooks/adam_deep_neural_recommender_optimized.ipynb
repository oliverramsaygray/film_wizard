{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f1498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bfd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de7b4b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:24:20.867055Z",
     "start_time": "2025-03-15T16:17:15.922690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 127ms/step - loss: 12.9067 - mae: 3.0387 - val_loss: 4.9650 - val_mae: 1.8916\n",
      "Epoch 2/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 106ms/step - loss: 3.5224 - mae: 1.3307 - val_loss: 1.3583 - val_mae: 0.7941\n",
      "Epoch 3/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 101ms/step - loss: 1.1761 - mae: 0.8426 - val_loss: 0.7163 - val_mae: 0.7561\n",
      "Epoch 4/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 108ms/step - loss: 0.6803 - mae: 0.7827 - val_loss: 0.5637 - val_mae: 0.7479\n",
      "Epoch 5/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 110ms/step - loss: 0.5412 - mae: 0.7490 - val_loss: 0.5218 - val_mae: 0.7548\n",
      "Epoch 6/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 105ms/step - loss: 0.4845 - mae: 0.7276 - val_loss: 0.5069 - val_mae: 0.7680\n",
      "Epoch 7/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 105ms/step - loss: 0.4530 - mae: 0.7135 - val_loss: 0.5023 - val_mae: 0.7844\n",
      "Epoch 8/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 106ms/step - loss: 0.4305 - mae: 0.7023 - val_loss: 0.4972 - val_mae: 0.7911\n",
      "Epoch 9/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 103ms/step - loss: 0.4144 - mae: 0.6941 - val_loss: 0.4901 - val_mae: 0.7908\n",
      "Epoch 10/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 109ms/step - loss: 0.4016 - mae: 0.6876 - val_loss: 0.4832 - val_mae: 0.7884\n",
      "Epoch 11/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 107ms/step - loss: 0.3924 - mae: 0.6828 - val_loss: 0.4758 - val_mae: 0.7826\n",
      "Epoch 12/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 106ms/step - loss: 0.3847 - mae: 0.6782 - val_loss: 0.4442 - val_mae: 0.7432\n",
      "Epoch 13/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 111ms/step - loss: 0.3784 - mae: 0.6750 - val_loss: 0.3726 - val_mae: 0.6661\n",
      "Epoch 14/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 106ms/step - loss: 0.3689 - mae: 0.6666 - val_loss: 0.4002 - val_mae: 0.7019\n",
      "Epoch 15/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 98ms/step - loss: 0.3642 - mae: 0.6640 - val_loss: 0.4413 - val_mae: 0.7466\n",
      "Epoch 16/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 98ms/step - loss: 0.3598 - mae: 0.6613 - val_loss: 0.4410 - val_mae: 0.7484\n",
      "Epoch 17/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 97ms/step - loss: 0.3562 - mae: 0.6583 - val_loss: 0.4475 - val_mae: 0.7595\n",
      "Epoch 18/20\n",
      "\u001b[1m219/219\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 100ms/step - loss: 0.3505 - mae: 0.6521 - val_loss: 0.4525 - val_mae: 0.7696\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "RMSE: 0.8711\n",
      "MAE: 0.6775\n",
      "R2 Score: 0.0904\n",
      "Diversity Score: 0.3739\n",
      "No recommendations available for User 123\n"
     ]
    }
   ],
   "source": [
    "# Updated Code with Areas for Consideration\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from google.cloud import bigquery\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, precision_score, recall_score\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "# Initialize BigQuery client\n",
    "PROJECT_ID = \"film-wizard-453315\"\n",
    "DATASET_ID = \"Grouplens\"\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Fetch data in batches from BigQuery to avoid memory issues\n",
    "# Scalability: This prevents excessive memory usage\n",
    "\n",
    "def fetch_data(batch_size=250000):\n",
    "    query = f'''\n",
    "    SELECT userId, movieId, rating \n",
    "    FROM `{PROJECT_ID}.{DATASET_ID}.raw_grouplens_ratings`\n",
    "    LIMIT {batch_size}\n",
    "    '''\n",
    "    return client.query(query).to_dataframe()\n",
    "\n",
    "# Load data\n",
    "ratings_df = fetch_data()\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Create User & Movie ID mappings\n",
    "user_to_idx = {user: i for i, user in enumerate(ratings_df['userId'].unique())}\n",
    "movie_to_idx = {movie: i for i, movie in enumerate(ratings_df['movieId'].unique())}\n",
    "\n",
    "ratings_df['user_idx'] = ratings_df['userId'].map(user_to_idx)\n",
    "ratings_df['movie_idx'] = ratings_df['movieId'].map(movie_to_idx)\n",
    "\n",
    "# Define Data Generator for Training\n",
    "\n",
    "def data_generator(data, batch_size=1024):\n",
    "    while True:\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data.iloc[i:i+batch_size]\n",
    "            X_batch = (batch['user_idx'].values, batch['movie_idx'].values)\n",
    "            y_batch = batch['rating'].values\n",
    "            yield X_batch, y_batch  # Ensure correct tuple format\n",
    "\n",
    "# Define Deep Learning Model\n",
    "num_users = len(user_to_idx)\n",
    "num_movies = len(movie_to_idx)\n",
    "embedding_size = 150  # Increased for better learning\n",
    "\n",
    "user_input = keras.Input(shape=(1,))\n",
    "movie_input = keras.Input(shape=(1,))\n",
    "\n",
    "# Added L2 regularization to embeddings\n",
    "user_embedding = keras.layers.Embedding(input_dim=num_users, output_dim=embedding_size,\n",
    "                                        embeddings_regularizer=keras.regularizers.l2(0.001))(user_input)\n",
    "movie_embedding = keras.layers.Embedding(input_dim=num_movies, output_dim=embedding_size,\n",
    "                                         embeddings_regularizer=keras.regularizers.l2(0.001))(movie_input)\n",
    "\n",
    "user_vec = keras.layers.Flatten()(user_embedding)\n",
    "movie_vec = keras.layers.Flatten()(movie_embedding)\n",
    "\n",
    "concat = keras.layers.Concatenate()([user_vec, movie_vec])\n",
    "dense1 = keras.layers.Dense(128, activation='relu')(concat)\n",
    "dropout1 = keras.layers.Dropout(0.4)(dense1)  # Increased dropout for regularization\n",
    "dense2 = keras.layers.Dense(64, activation='relu')(dropout1)\n",
    "dropout2 = keras.layers.Dropout(0.4)(dense2)  # Increased dropout for better generalization\n",
    "output = keras.layers.Dense(1, activation='linear')(dropout2)\n",
    "\n",
    "model = keras.Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss=tf.keras.losses.Huber(), metrics=['mae'])  # Lower learning rate\n",
    "\n",
    "# Add Early Stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train Model with Batch Processing\n",
    "batch_size = 1024\n",
    "train_steps = len(ratings_df) // batch_size\n",
    "\n",
    "# Split data into training (90%) and validation (10%)\n",
    "train_data = ratings_df.sample(frac=0.9, random_state=42)\n",
    "val_data = ratings_df.drop(train_data.index)\n",
    "\n",
    "model.fit(\n",
    "    data_generator(train_data, batch_size),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    validation_data=data_generator(val_data, batch_size),\n",
    "    validation_steps=len(val_data) // batch_size,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Additional Evaluation Metrics\n",
    "# 1. Precision & Recall\n",
    "# 2. Diversity & Serendipity - To ensure recommendations are not too homogeneous\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n",
    "# Generate predictions using batch processing\n",
    "val_users = val_data['user_idx'].values\n",
    "val_movies = val_data['movie_idx'].values\n",
    "\n",
    "# Predict in batches to reduce excessive print logs\n",
    "predicted_ratings = model.predict([val_users, val_movies], batch_size=1024).flatten()\n",
    "\n",
    "# Ensure y_pred has the correct shape\n",
    "y_true = val_data['rating'].values\n",
    "y_pred = predicted_ratings.reshape(-1)  # Flatten in case of incorrect shape\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compute RMSE manually\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Combine predictions with actual values\n",
    "predictions = list(zip(val_data['userId'], val_data['movieId'], val_data['rating'], y_pred, [None]*len(val_data)))\n",
    "\n",
    "# Compute top recommendations\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Compute diversity: Measure how varied recommendations are\n",
    "unique_movies = set()\n",
    "for uid, user_ratings in top_n.items():\n",
    "    unique_movies.update([iid for iid, _ in user_ratings])\n",
    "diversity_score = len(unique_movies) / len(set(ratings_df['movieId']))\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")\n",
    "print(f\"Diversity Score: {diversity_score:.4f}\")\n",
    "\n",
    "# Explanation Component: Justifying recommendations\n",
    "# This helps users understand why a particular recommendation was made\n",
    "\n",
    "def explain_recommendation(user_id, top_n, ratings_df):\n",
    "    if user_id not in top_n:\n",
    "        print(f\"No recommendations available for User {user_id}\")\n",
    "        return\n",
    "    print(f\"User {user_id} recommendations:\")\n",
    "    for movie_id, score in top_n[user_id]:\n",
    "        movie_title = ratings_df.loc[ratings_df['movieId'] == movie_id, 'title'].values\n",
    "        print(f\"- {movie_title[0] if len(movie_title) > 0 else movie_id} (Score: {score:.2f})\")\n",
    "\n",
    "# Example explanation for a user\n",
    "explain_recommendation(123, top_n, ratings_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aafb02d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:27:42.319937Z",
     "start_time": "2025-03-15T16:27:42.289655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ **Final Model Evaluation Summary** ðŸŽ¯\n",
      "Test RMSE: 0.8711\n",
      "Test MAE: 0.6775\n",
      "RÂ² Score: 0.0904\n",
      "\n",
      "ðŸ“Š **Average Predicted Rating:** 3.10\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Print Summary of Results\n",
    "print(\"\\nðŸŽ¯ **Final Model Evaluation Summary** ðŸŽ¯\")\n",
    "# print(f\"Test MSE: {mse:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "\n",
    "# Compute the average predicted rating\n",
    "avg_pred_rating = np.mean(predicted_ratings)\n",
    "print(f\"\\nðŸ“Š **Average Predicted Rating:** {avg_pred_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77161fc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:31:41.384362Z",
     "start_time": "2025-03-15T16:31:39.514950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No movies found in the test dataset after filtering. Check IMDb ID matching.\n"
     ]
    }
   ],
   "source": [
    "#Check my ratings vs predicted ratings\n",
    "\n",
    "# Initialize BigQuery client\n",
    "PROJECT_ID = \"film-wizard-453315\"\n",
    "DATASET_ID = \"Grouplens\"\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# ðŸš€ **STEP 2: TEST THE MODEL ON SEPARATE IMDb CSV**\n",
    "\n",
    "# Load IMDb Ratings CSV\n",
    "my_csv = '/Users/adamdyerson/Downloads/IMDB My Ratings.csv'\n",
    "your_ratings = pd.read_csv(my_csv)\n",
    "\n",
    "# Rename columns to match expected format (if necessary)\n",
    "your_ratings.rename(columns={'Your Rating': 'rating', 'Const': 'imdbId'}, inplace=True)\n",
    "\n",
    "# Fetch `movies_with_imdb` to map IMDb to MovieLens IDs\n",
    "query_movies = '''\n",
    "SELECT movieId, title, imdbId, tmdbId \n",
    "FROM `film-wizard-453315.Grouplens.movies_with_imdb`\n",
    "'''\n",
    "movies_with_imdb = client.query(query_movies).to_dataframe()\n",
    "\n",
    "# Ensure IMDb IDs are formatted properly\n",
    "your_ratings['imdbId'] = your_ratings['imdbId'].astype(str).str.zfill(7)\n",
    "movies_with_imdb['imdbId'] = movies_with_imdb['imdbId'].astype(str).str.zfill(7)\n",
    "\n",
    "# Match IMDb Ratings to MovieLens IDs (Only for Testing)\n",
    "test_ratings = your_ratings.merge(movies_with_imdb, on=\"imdbId\", how=\"inner\")\n",
    "test_ratings = test_ratings[['movieId', 'title', 'rating']]\n",
    "\n",
    "# Convert IMDb Test Data to Model Format\n",
    "test_ratings['user_idx'] = 0  # Dummy user ID\n",
    "test_ratings['movie_idx'] = test_ratings['movieId'].map(movie_to_idx)\n",
    "\n",
    "# Remove any movies not in training data\n",
    "test_ratings = test_ratings.dropna(subset=['movie_idx'])\n",
    "\n",
    "# Ensure test set is not empty before making predictions\n",
    "if test_ratings.empty:\n",
    "    print(\"âš ï¸ No movies found in the test dataset after filtering. Check IMDb ID matching.\")\n",
    "else:\n",
    "    # Make Predictions for IMDb Movies\n",
    "    X_test = [test_ratings['user_idx'].values, test_ratings['movie_idx'].values]\n",
    "    predicted_ratings = model.predict(X_test, batch_size=1024).flatten()\n",
    "\n",
    "    # Attach Predictions & Sort\n",
    "    test_ratings['predicted_rating'] = predicted_ratings\n",
    "    test_ratings = test_ratings.sort_values(by='predicted_rating', ascending=False)\n",
    "\n",
    "    print(\"ðŸŽ¬ **Predicted Ratings for Your IMDb Movies** ðŸŽ¬\")\n",
    "    print(test_ratings[['title', 'rating', 'predicted_rating']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2817a032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:32:47.888366Z",
     "start_time": "2025-03-15T16:32:47.863614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Ratings IMDb IDs: ['tt7671070' 'tt0118715' 'tt0034583' 'tt29940008' 'tt7203552' 'tt11691774'\n",
      " 'tt9051908' 'tt0095765' 'tt13406094' 'tt3470600']\n",
      "Movies with IMDb IDs: ['5504168' '6268930' '0371501' '6917242' '11898442' '6756498' '0986232'\n",
      " '1754898' '0000192' '0352691']\n"
     ]
    }
   ],
   "source": [
    "print(\"Your Ratings IMDb IDs:\", your_ratings['imdbId'].unique()[:10])\n",
    "print(\"Movies with IMDb IDs:\", movies_with_imdb['imdbId'].unique()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0400a432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:34:52.518228Z",
     "start_time": "2025-03-15T16:34:50.356951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "ðŸŽ¬ **Predicted Ratings for Your IMDb Movies** ðŸŽ¬\n",
      "                                                title  rating  \\\n",
      "35  For a Few Dollars More (Per qualche dollaro in...     4.0   \n",
      "99                    One Flew Over the Cuckoo's Nest     5.0   \n",
      "6                           Shawshank Redemption, The     5.0   \n",
      "61                                     Godfather, The     5.0   \n",
      "12                                         Goodfellas     5.0   \n",
      "..                                                ...     ...   \n",
      "13                               Mary Poppins Returns     3.5   \n",
      "28                                           The Game     2.5   \n",
      "80                                         Uncle Buck     4.0   \n",
      "52                                     Coogan's Bluff     3.0   \n",
      "37                                      Sudden Impact     3.5   \n",
      "\n",
      "    predicted_rating  \n",
      "35          4.007880  \n",
      "99          4.000143  \n",
      "6           3.984360  \n",
      "61          3.940126  \n",
      "12          3.924551  \n",
      "..               ...  \n",
      "13          2.982762  \n",
      "28          2.971961  \n",
      "80          2.896833  \n",
      "52          2.862234  \n",
      "37          2.705493  \n",
      "\n",
      "[106 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize BigQuery client\n",
    "PROJECT_ID = \"film-wizard-453315\"\n",
    "DATASET_ID = \"Grouplens\"\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# ðŸš€ **STEP 2: TEST THE MODEL ON SEPARATE IMDb CSV**\n",
    "\n",
    "# Load IMDb Ratings CSV\n",
    "my_csv = '/Users/adamdyerson/Downloads/IMDB My Ratings.csv'\n",
    "your_ratings = pd.read_csv(my_csv)\n",
    "\n",
    "# Rename columns to match expected format (if necessary)\n",
    "your_ratings.rename(columns={'Your Rating': 'rating', 'Const': 'imdbId'}, inplace=True)\n",
    "\n",
    "# Fetch `movies_with_imdb` to map IMDb to MovieLens IDs\n",
    "query_movies = '''\n",
    "SELECT movieId, title, imdbId, tmdbId \n",
    "FROM `film-wizard-453315.Grouplens.movies_with_imdb`\n",
    "'''\n",
    "movies_with_imdb = client.query(query_movies).to_dataframe()\n",
    "\n",
    "# Ensure IMDb IDs are formatted properly\n",
    "your_ratings['imdbId'] = your_ratings['imdbId'].astype(str).str.zfill(7).str.replace('tt', '')\n",
    "movies_with_imdb['imdbId'] = movies_with_imdb['imdbId'].astype(str).str.zfill(7)\n",
    "\n",
    "# Match IMDb Ratings to MovieLens IDs (Only for Testing)\n",
    "test_ratings = your_ratings.merge(movies_with_imdb, on=\"imdbId\", how=\"inner\")\n",
    "test_ratings = test_ratings[['movieId', 'title', 'rating']]\n",
    "\n",
    "# Convert IMDb ratings from 10-point scale to 5-point scale\n",
    "test_ratings['rating'] = test_ratings['rating'] / 2\n",
    "\n",
    "# Convert IMDb Test Data to Model Format\n",
    "test_ratings['user_idx'] = 0  # Dummy user ID\n",
    "test_ratings['movie_idx'] = test_ratings['movieId'].map(movie_to_idx)\n",
    "\n",
    "# Remove any movies not in training data\n",
    "test_ratings = test_ratings.dropna(subset=['movie_idx'])\n",
    "\n",
    "# Ensure test set is not empty before making predictions\n",
    "if test_ratings.empty:\n",
    "    print(\"âš ï¸ No movies found in the test dataset after filtering. Check IMDb ID matching.\")\n",
    "else:\n",
    "    # Make Predictions for IMDb Movies\n",
    "    X_test = [test_ratings['user_idx'].values, test_ratings['movie_idx'].values]\n",
    "    predicted_ratings = model.predict(X_test, batch_size=1024).flatten()\n",
    "\n",
    "    # Attach Predictions & Sort\n",
    "    test_ratings['predicted_rating'] = predicted_ratings\n",
    "    test_ratings = test_ratings.sort_values(by='predicted_rating', ascending=False)\n",
    "\n",
    "    print(\"ðŸŽ¬ **Predicted Ratings for Your IMDb Movies** ðŸŽ¬\")\n",
    "    print(test_ratings[['title', 'rating', 'predicted_rating']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b66761ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T16:57:00.283061Z",
     "start_time": "2025-03-15T16:56:46.912048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ **Recommended Movies Based on Similar Users** ðŸŽ¥\n",
      "                                                   title  rating  \\\n",
      "57824                                     Carry on Jatta     5.0   \n",
      "72674                                   Peg of Old Drury     5.0   \n",
      "29274                                    The Unfaithfuls     5.0   \n",
      "72612                                 Â¡Se armÃ³ el belÃ©n!     5.0   \n",
      "29240                                Jailbait Babysitter     5.0   \n",
      "29235                                        Blue Summer     5.0   \n",
      "72650  Music with Roots in the Aether: Opera for Tele...     5.0   \n",
      "62980  The Incredible Adventure of Jojo (And His Anno...     5.0   \n",
      "62871                                           Ciao Ni!     5.0   \n",
      "64207                           AC/DC: Live At Donington     5.0   \n",
      "\n",
      "                                               reason  \n",
      "57824  Rated highly (5.00) by 2 users similar to you.  \n",
      "72674  Rated highly (5.00) by 1 users similar to you.  \n",
      "29274  Rated highly (5.00) by 1 users similar to you.  \n",
      "72612  Rated highly (5.00) by 1 users similar to you.  \n",
      "29240  Rated highly (5.00) by 1 users similar to you.  \n",
      "29235  Rated highly (5.00) by 1 users similar to you.  \n",
      "72650  Rated highly (5.00) by 1 users similar to you.  \n",
      "62980  Rated highly (5.00) by 1 users similar to you.  \n",
      "62871  Rated highly (5.00) by 2 users similar to you.  \n",
      "64207  Rated highly (5.00) by 1 users similar to you.  \n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ **STEP 3: RECOMMEND NEW MOVIES BASED ON SIMILAR USERS**\n",
    "\n",
    "# Fetch user-movie interactions from the dataset\n",
    "query_ratings = '''\n",
    "SELECT userId, movieId, rating \n",
    "FROM `film-wizard-453315.Grouplens.raw_grouplens_ratings`\n",
    "'''\n",
    "ratings_data = client.query(query_ratings).to_dataframe()\n",
    "\n",
    "# Identify similar users based on past ratings\n",
    "similar_users = ratings_data[ratings_data['movieId'].isin(test_ratings['movieId'])]\n",
    "similar_users = similar_users.groupby('userId').filter(lambda x: len(x) > 6)  # Reduce threshold to find more similar users  # Users who rated at least 5 of the same movies\n",
    "\n",
    "# Get movie recommendations from these users\n",
    "recommended_movies = ratings_data[ratings_data['userId'].isin(similar_users['userId'])]\n",
    "recommended_movies = recommended_movies.groupby('movieId').agg({'rating': 'mean', 'userId': 'count'}).reset_index()\n",
    "\n",
    "# Merge with movie titles\n",
    "recommended_movies = recommended_movies.merge(movies_with_imdb, on='movieId', how='left')\n",
    "recommended_movies = recommended_movies[['title', 'rating', 'userId']]\n",
    "\n",
    "# Filter out low-rated recommendations\n",
    "#This gives rating that similar user gave it, not IMDB rating\n",
    "recommended_movies = recommended_movies[recommended_movies['rating'] >= 4].sort_values(by='rating', ascending=False).head(10)\n",
    "\n",
    "# Add explanation for recommendations\n",
    "recommended_movies.rename(columns={'userId': 'num_users_rated'}, inplace=True)\n",
    "recommended_movies['reason'] = recommended_movies.apply(lambda row: f\"Rated highly ({row['rating']:.2f}) by {row['num_users_rated']} users similar to you.\", axis=1)\n",
    "\n",
    "print(\"ðŸŽ¥ **Recommended Movies Based on Similar Users** ðŸŽ¥\")\n",
    "print(recommended_movies[['title', 'rating', 'reason']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607c949",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
