{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2920e5",
   "metadata": {},
   "source": [
    "# ANN Model: Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609ce98a",
   "metadata": {},
   "source": [
    "# Claude Improvements and Corrections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a8ef7e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T15:12:17.491573Z",
     "start_time": "2025-03-16T14:38:44.380584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Fetched 100000 rows\n",
      "Fetched 200000 rows\n",
      "Fetched 300000 rows\n",
      "Fetched 400000 rows\n",
      "Fetched 500000 rows\n",
      "Fetched 600000 rows\n",
      "Fetched 700000 rows\n",
      "Fetched 800000 rows\n",
      "Fetched 900000 rows\n",
      "Fetched 1000000 rows\n",
      "Fetched 1100000 rows\n",
      "Fetched 1200000 rows\n",
      "Fetched 1300000 rows\n",
      "Fetched 1400000 rows\n",
      "Fetched 1500000 rows\n",
      "Fetched 1600000 rows\n",
      "Fetched 1700000 rows\n",
      "Fetched 1800000 rows\n",
      "Fetched 1900000 rows\n",
      "Fetched 2000000 rows\n",
      "Raw data: 2000000 ratings\n",
      "Filtered data: 1758095 ratings, 102658 users, 10418 movies\n",
      "Building FAISS HNSW index...\n",
      "Added users up to 10999\n",
      "Added users up to 20999\n",
      "Added users up to 30999\n",
      "Added users up to 40999\n",
      "Added users up to 50999\n",
      "Added users up to 60999\n",
      "Added users up to 70999\n",
      "Added users up to 80999\n",
      "Added users up to 90999\n",
      "Added users up to 100999\n",
      "FAISS indexing complete\n",
      "Predicting ratings for test set...\n",
      "Processed 0 users\n",
      "Processed 1000 users\n",
      "Processed 2000 users\n",
      "Processed 3000 users\n",
      "Processed 4000 users\n",
      "Processed 5000 users\n",
      "Processed 6000 users\n",
      "Processed 7000 users\n",
      "Processed 8000 users\n",
      "Processed 9000 users\n",
      "Processed 10000 users\n",
      "Processed 11000 users\n",
      "Processed 12000 users\n",
      "Processed 13000 users\n",
      "Processed 14000 users\n",
      "Processed 15000 users\n",
      "Processed 16000 users\n",
      "Processed 17000 users\n",
      "Processed 18000 users\n",
      "Processed 19000 users\n",
      "Processed 20000 users\n",
      "Processed 21000 users\n",
      "Processed 22000 users\n",
      "Processed 23000 users\n",
      "Processed 24000 users\n",
      "Processed 25000 users\n",
      "Processed 26000 users\n",
      "Processed 27000 users\n",
      "Processed 28000 users\n",
      "Processed 29000 users\n",
      "Processed 30000 users\n",
      "Processed 31000 users\n",
      "Processed 32000 users\n",
      "Processed 33000 users\n",
      "Processed 34000 users\n",
      "Processed 35000 users\n",
      "Processed 36000 users\n",
      "Processed 37000 users\n",
      "Processed 38000 users\n",
      "Processed 39000 users\n",
      "Processed 40000 users\n",
      "Processed 41000 users\n",
      "Processed 42000 users\n",
      "Processed 43000 users\n",
      "Processed 44000 users\n",
      "Processed 45000 users\n",
      "Processed 46000 users\n",
      "Processed 47000 users\n",
      "Processed 48000 users\n",
      "Processed 49000 users\n",
      "Processed 50000 users\n",
      "Processed 51000 users\n",
      "Processed 52000 users\n",
      "Processed 53000 users\n",
      "Processed 54000 users\n",
      "Processed 55000 users\n",
      "Processed 56000 users\n",
      "Processed 57000 users\n",
      "Processed 58000 users\n",
      "Processed 59000 users\n",
      "Processed 60000 users\n",
      "Processed 61000 users\n",
      "Processed 62000 users\n",
      "Processed 63000 users\n",
      "Processed 64000 users\n",
      "Processed 65000 users\n",
      "Processed 66000 users\n",
      "Processed 67000 users\n",
      "Processed 68000 users\n",
      "Processed 69000 users\n",
      "Processed 70000 users\n",
      "Processed 71000 users\n",
      "Processed 72000 users\n",
      "Processed 73000 users\n",
      "Processed 74000 users\n",
      "Processed 75000 users\n",
      "Processed 76000 users\n",
      "Processed 77000 users\n",
      "Processed 78000 users\n",
      "Processed 79000 users\n",
      "Processed 80000 users\n",
      "Processed 81000 users\n",
      "Processed 82000 users\n",
      "Processed 83000 users\n",
      "Processed 84000 users\n",
      "Processed 85000 users\n",
      "Processed 86000 users\n",
      "Processed 87000 users\n",
      "Processed 88000 users\n",
      "Processed 89000 users\n",
      "\n",
      "Results:\n",
      "Z-Score RMSE: 0.8938\n",
      "Z-Score MAE: 0.5339\n",
      "Rating RMSE: 0.6249\n",
      "Number of predictions: 6242\n",
      "\n",
      "Sample predictions:\n",
      "User 152354.0, Movie 220380.0: Actual 4.13, Predicted 4.97\n",
      "User 185665.0, Movie 105844.0: Actual 4.07, Predicted 4.07\n",
      "User 20590.0, Movie 306.0: Actual 4.65, Predicted 4.43\n",
      "User 124700.0, Movie 62644.0: Actual 2.39, Predicted 2.39\n",
      "User 159663.0, Movie 166635.0: Actual 1.95, Predicted 2.31\n",
      "Total execution time: 2013.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from google.cloud import bigquery\n",
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "import faiss  # Approximate Nearest Neighbors\n",
    "\n",
    "# Initialize BigQuery client\n",
    "PROJECT_ID = \"film-wizard-453315\"\n",
    "DATASET_ID = \"Grouplens\"\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Fetch data in batches from BigQuery\n",
    "def fetch_data(batch_size=100000, max_rows=2000000):\n",
    "    try:\n",
    "        offset = 0\n",
    "        all_data = []\n",
    "        \n",
    "        while offset < max_rows:\n",
    "            query = f'''\n",
    "            SELECT userId, movieId, rating \n",
    "            FROM `{PROJECT_ID}.{DATASET_ID}.raw_grouplens_ratings`\n",
    "            LIMIT {batch_size} OFFSET {offset}\n",
    "            '''\n",
    "            df = client.query(query).to_dataframe()\n",
    "            if df.empty:\n",
    "                break\n",
    "                \n",
    "            all_data.append(df)\n",
    "            offset += batch_size\n",
    "            # Only print every 100k rows\n",
    "            if offset % 100000 == 0:\n",
    "                print(f\"Fetched {offset} rows\")\n",
    "            \n",
    "        return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to normalize ratings using Z-score (only on training data)\n",
    "def normalize_ratings(df):\n",
    "    user_means = df.groupby('userId')['rating'].mean()\n",
    "    user_stds = df.groupby('userId')['rating'].std().fillna(1.0)  # Handle users with constant ratings\n",
    "    \n",
    "    # Merge means and stds back to df\n",
    "    df = df.join(user_means.rename('user_mean'), on='userId')\n",
    "    df = df.join(user_stds.rename('user_std'), on='userId')\n",
    "    \n",
    "    # Calculate z_score\n",
    "    df['z_score'] = (df['rating'] - df['user_mean']) / df['user_std']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply user means and stds from training set to test set\n",
    "def apply_normalization(test_df, train_df):\n",
    "    # Extract user stats from training data\n",
    "    user_means = train_df.groupby('userId')['rating'].mean()\n",
    "    user_stds = train_df.groupby('userId')['rating'].std().fillna(1.0)\n",
    "    \n",
    "    # Apply to test data\n",
    "    test_df = test_df.join(user_means.rename('user_mean'), on='userId')\n",
    "    test_df = test_df.join(user_stds.rename('user_std'), on='userId')\n",
    "    \n",
    "    # Handle users not in training set - fixing the warnings\n",
    "    global_mean = train_df['rating'].mean()\n",
    "    global_std = train_df['rating'].std()\n",
    "    \n",
    "    # Fix pandas warning by avoiding chained assignment\n",
    "    test_df.loc[test_df['user_mean'].isna(), 'user_mean'] = global_mean\n",
    "    test_df.loc[test_df['user_std'].isna(), 'user_std'] = global_std\n",
    "    \n",
    "    # Calculate z_score\n",
    "    test_df['z_score'] = (test_df['rating'] - test_df['user_mean']) / test_df['user_std']\n",
    "    \n",
    "    return test_df\n",
    "\n",
    "# Main process\n",
    "def main():\n",
    "    # Load data\n",
    "    print(\"Fetching data...\")\n",
    "    try:\n",
    "        data = fetch_data(batch_size=100000, max_rows=2000000)  # Amend data size as required\n",
    "        if data.empty:\n",
    "            print(\"No data retrieved. Exiting.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Raw data: {len(data)} ratings\")\n",
    "    \n",
    "    # Filter to keep only top users and popular movies\n",
    "    user_counts = data['userId'].value_counts()\n",
    "    movie_counts = data['movieId'].value_counts()\n",
    "    \n",
    "    top_users = user_counts[user_counts >= 5].index  # Reduced minimum ratings per user\n",
    "    top_movies = movie_counts[movie_counts >= 10].index  # Reduced minimum ratings per movie\n",
    "    \n",
    "    data = data[data['userId'].isin(top_users)]\n",
    "    data = data[data['movieId'].isin(top_movies)]\n",
    "    \n",
    "    print(f\"Filtered data: {len(data)} ratings, {len(top_users)} users, {len(top_movies)} movies\")\n",
    "    \n",
    "    # Train-test split (do this before normalization to prevent data leakage)\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Normalize ratings using z-score (training data only)\n",
    "    train_data = normalize_ratings(train_data)\n",
    "    \n",
    "    # Apply normalization from training data to test data\n",
    "    test_data = apply_normalization(test_data, train_data)\n",
    "    \n",
    "    # Build sparse matrix for training data\n",
    "    user_ids = sorted(train_data['userId'].unique())\n",
    "    movie_ids = sorted(train_data['movieId'].unique())\n",
    "    \n",
    "    user_idx_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "    movie_idx_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "    \n",
    "    # Create sparse matrix\n",
    "    row_idx = [user_idx_map[uid] for uid in train_data['userId']]\n",
    "    col_idx = [movie_idx_map[mid] for mid in train_data['movieId']]\n",
    "    ratings = train_data['z_score'].values\n",
    "    \n",
    "    user_movie_matrix = coo_matrix((ratings, (row_idx, col_idx)), \n",
    "                                   shape=(len(user_ids), len(movie_ids)))\n",
    "    user_movie_csr = user_movie_matrix.tocsr()\n",
    "    \n",
    "    # Build FAISS HNSW index\n",
    "    print(\"Building FAISS HNSW index...\")\n",
    "    n_users, n_movies = user_movie_csr.shape\n",
    "    \n",
    "    try:\n",
    "        d = n_movies  # Dimensions = number of movies\n",
    "        faiss_index = faiss.IndexHNSWFlat(d, 32)  # 32 links per node\n",
    "        \n",
    "        # Convert sparse to dense in batches to avoid memory issues\n",
    "        batch_size = 1000  # Adjust based on available RAM\n",
    "        for start_idx in range(0, n_users, batch_size):\n",
    "            end_idx = min(start_idx + batch_size, n_users)\n",
    "            batch = user_movie_csr[start_idx:end_idx].toarray().astype('float32')\n",
    "            \n",
    "            # Replace NaN values (FAISS doesn't handle NaN)\n",
    "            batch = np.nan_to_num(batch, nan=0.0)\n",
    "            \n",
    "            faiss_index.add(batch)\n",
    "            # Only print every 10k users\n",
    "            if (end_idx - 1) % 10000 < batch_size and end_idx > 10000:\n",
    "                print(f\"Added users up to {end_idx-1}\")\n",
    "        \n",
    "        print(\"FAISS indexing complete\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error building FAISS index: {e}\")\n",
    "        return\n",
    "        \n",
    "    # Predict ratings for test set\n",
    "    print(\"Predicting ratings for test set...\")\n",
    "    predictions = []\n",
    "    \n",
    "    try:\n",
    "        # Group test data by user for more efficient processing\n",
    "        user_groups = test_data.groupby('userId')\n",
    "        processed_users = 0\n",
    "        \n",
    "        for user_id, group in user_groups:\n",
    "            if processed_users % 1000 == 0:\n",
    "                print(f\"Processed {processed_users} users\")\n",
    "            processed_users += 1\n",
    "            \n",
    "            if user_id in user_idx_map:\n",
    "                # Get user vector from CSR matrix\n",
    "                user_idx = user_idx_map[user_id]\n",
    "                user_vector = user_movie_csr[user_idx].toarray().astype('float32')\n",
    "                \n",
    "                # Handle zero vectors (users with no ratings)\n",
    "                if user_vector.sum() == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Replace NaN values\n",
    "                user_vector = np.nan_to_num(user_vector, nan=0.0)\n",
    "                \n",
    "                # Find similar users\n",
    "                try:\n",
    "                    D, I = faiss_index.search(user_vector, k=50)  #50 nearest neighbors gave better results than 20\n",
    "                except Exception as e:\n",
    "                    print(f\"FAISS search error for user {user_id}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each movie the user rated in test set. Different weights not yet tested.\n",
    "                for _, row in group.iterrows():\n",
    "                    movie_id = row['movieId']\n",
    "                    if movie_id in movie_idx_map:\n",
    "                        movie_idx = movie_idx_map[movie_id]\n",
    "                        \n",
    "                        # Get ratings from similar users for this movie\n",
    "                        similar_ratings = []\n",
    "                        similar_distances = []\n",
    "                        for i, neighbor_idx in enumerate(I[0]):\n",
    "                            if 0 <= neighbor_idx < n_users:  # Valid index\n",
    "                                rating = user_movie_csr[neighbor_idx, movie_idx]\n",
    "                                if rating != 0:  # Rating exists\n",
    "                                    similar_ratings.append(float(rating))\n",
    "                                    similar_distances.append(D[0][i])\n",
    "                        \n",
    "                        if similar_ratings:\n",
    "                            # Fix weights sum to zero issue\n",
    "                            weights = np.exp(-np.array(similar_distances))\n",
    "                            \n",
    "                            # Check if weights sum to zero and fix if needed\n",
    "                            if weights.sum() == 0:\n",
    "                                # Use a simple average instead\n",
    "                                weighted_avg = np.mean(similar_ratings)\n",
    "                            else:\n",
    "                                # Use weighted average\n",
    "                                weighted_avg = np.average(similar_ratings, weights=weights)\n",
    "                            \n",
    "                            # Ensure we have a finite value\n",
    "                            if np.isfinite(weighted_avg) and np.isfinite(row['z_score']):\n",
    "                                # Store prediction\n",
    "                                predictions.append({\n",
    "                                    'userId': user_id,\n",
    "                                    'movieId': movie_id,\n",
    "                                    'actual': float(row['z_score']),\n",
    "                                    'predicted': float(weighted_avg)\n",
    "                                })\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions)\n",
    "        \n",
    "        # Evaluate model\n",
    "        if not predictions_df.empty:\n",
    "            # Check for and remove any remaining NaN values\n",
    "            predictions_df = predictions_df.dropna(subset=['actual', 'predicted'])\n",
    "            \n",
    "            if len(predictions_df) > 0:\n",
    "                rmse = np.sqrt(mean_squared_error(predictions_df['actual'], predictions_df['predicted']))\n",
    "                mae = mean_absolute_error(predictions_df['actual'], predictions_df['predicted'])\n",
    "                \n",
    "                # Convert z-scores back to ratings for interpretability\n",
    "                # Do the merges first and explicit column access to avoid NaN issues\n",
    "                user_mean_df = predictions_df.merge(test_data[['userId', 'user_mean']], on='userId')\n",
    "                user_std_df = predictions_df.merge(test_data[['userId', 'user_std']], on='userId')\n",
    "                \n",
    "                predictions_df['actual_rating'] = (predictions_df['actual'] * \n",
    "                                                  user_std_df['user_std'] + \n",
    "                                                  user_mean_df['user_mean'])\n",
    "                \n",
    "                predictions_df['predicted_rating'] = (predictions_df['predicted'] * \n",
    "                                                    user_std_df['user_std'] + \n",
    "                                                    user_mean_df['user_mean'])\n",
    "                \n",
    "                # Clip predictions to valid rating range and handle any NaNs\n",
    "                predictions_df['predicted_rating'] = predictions_df['predicted_rating'].clip(1, 5)\n",
    "                predictions_df = predictions_df.dropna(subset=['actual_rating', 'predicted_rating'])\n",
    "                \n",
    "                if len(predictions_df) > 0:\n",
    "                    rmse_raw = np.sqrt(mean_squared_error(predictions_df['actual_rating'], \n",
    "                                                        predictions_df['predicted_rating']))\n",
    "                    \n",
    "                    print(f\"\\nResults:\")\n",
    "                    print(f\"Z-Score RMSE: {rmse:.4f}\")\n",
    "                    print(f\"Z-Score MAE: {mae:.4f}\")\n",
    "                    print(f\"Rating RMSE: {rmse_raw:.4f}\")\n",
    "                    print(f\"Number of predictions: {len(predictions_df)}\")\n",
    "                    \n",
    "                    # Show a few sample predictions\n",
    "                    print(\"\\nSample predictions:\")\n",
    "                    sample = predictions_df.sample(5) if len(predictions_df) >= 5 else predictions_df\n",
    "                    for _, row in sample.iterrows():\n",
    "                        print(f\"User {row['userId']}, Movie {row['movieId']}: Actual {row['actual_rating']:.2f}, Predicted {row['predicted_rating']:.2f}\")\n",
    "                else:\n",
    "                    print(\"After cleaning NaN values, no valid predictions remain\")\n",
    "            else:\n",
    "                print(\"After cleaning NaN values, no valid predictions remain\")\n",
    "        else:\n",
    "            print(\"No predictions were generated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fdb659d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:38:30.305701Z",
     "start_time": "2025-03-16T17:38:30.277759Z"
    }
   },
   "outputs": [],
   "source": [
    "user_ids = sorted(train_data['userId'].unique())\n",
    "movie_ids = sorted(train_data['movieId'].unique())\n",
    "user_idx_map = {uid: idx for idx, uid in enumerate(user_ids)}\n",
    "movie_idx_map = {mid: idx for idx, mid in enumerate(movie_ids)}\n",
    "n_movies = len(movie_ids)  # Add this line for n_movies\n",
    "n_users = len(user_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b54c595",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:39:53.988998Z",
     "start_time": "2025-03-16T17:39:15.155816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the user-movie matrix (CSR format)\n",
    "# First create the COO matrix\n",
    "row_idx = [user_idx_map[uid] for uid in train_data['userId']]\n",
    "col_idx = [movie_idx_map[mid] for mid in train_data['movieId']]\n",
    "ratings = train_data['z_score'].values  # Use z-scores for better prediction\n",
    "\n",
    "user_movie_matrix = coo_matrix((ratings, (row_idx, col_idx)), \n",
    "                               shape=(len(user_ids), len(movie_ids)))\n",
    "user_movie_csr = user_movie_matrix.tocsr()\n",
    "\n",
    "# Also need to initialize FAISS index\n",
    "# This is a simplified version - in the original code it's more complex\n",
    "d = n_movies  # Dimensions = number of movies\n",
    "faiss_index = faiss.IndexHNSWFlat(d, 32)  # 32 links per node\n",
    "\n",
    "# Convert sparse to dense for FAISS indexing\n",
    "# In a real implementation, you'd do this in batches to avoid memory issues\n",
    "user_vectors = user_movie_csr.toarray().astype('float32')\n",
    "user_vectors = np.nan_to_num(user_vectors, nan=0.0)  # Replace NaN values\n",
    "faiss_index.add(user_vectors)  # Add vectors to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a6a9902",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:40:13.828881Z",
     "start_time": "2025-03-16T17:40:11.525160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 108 of your ratings in the MovieLens dataset\n",
      "\n",
      "Your Personal Recommendation Results:\n",
      "RMSE: 0.5060\n",
      "MAE: 0.3671\n",
      "Number of predictions: 12\n",
      "\n",
      "Biggest Rating Differences (Movies you might have unusual taste in):\n",
      "                        title  actual  predicted  difference\n",
      "6    Wolf of Wall Street, The     4.5   3.131709    1.368291\n",
      "0                  Casablanca     5.0   4.307172    0.692828\n",
      "1   Shawshank Redemption, The     5.0   4.470659    0.529341\n",
      "8               Departed, The     4.0   4.353638   -0.353638\n",
      "11               Garden State     2.5   2.837544   -0.337544\n",
      "\n",
      "Closest Rating Matches (Movies the system understands your taste in):\n",
      "                                               title  actual  predicted  \\\n",
      "9                                    American Beauty     5.0   4.941948   \n",
      "5   Birdman: Or (The Unexpected Virtue of Ignorance)     4.0   3.875277   \n",
      "10                                       Frost/Nixon     4.0   4.156893   \n",
      "4                                 The Imitation Game     4.0   4.163661   \n",
      "3                                   Django Unchained     4.0   4.172147   \n",
      "\n",
      "    difference  \n",
      "9     0.058052  \n",
      "5     0.124723  \n",
      "10   -0.156893  \n",
      "4    -0.163661  \n",
      "3    -0.172147  \n"
     ]
    }
   ],
   "source": [
    "#Testing on my csv which I can add to BQ later\n",
    "\n",
    "my_csv = '/Users/adamdyerson/Downloads/IMDB My Ratings.csv'\n",
    "user_ratings = pd.read_csv(my_csv)\n",
    "user_ratings.rename(columns={'Your Rating': 'rating', 'Const': 'imdbId'}, inplace=True)\n",
    "user_ratings['rating'] = user_ratings['rating'] / 2  # Convert 10-point to 5-point scale\n",
    "\n",
    "# Fetch movies mapping IMDb to MovieLens IDs\n",
    "query_movies = '''\n",
    "SELECT movieId, title, imdbId, tmdbId \n",
    "FROM `film-wizard-453315.Grouplens.movies_with_imdb`\n",
    "'''\n",
    "movies_with_imdb = client.query(query_movies).to_dataframe()\n",
    "\n",
    "# Normalize IMDb IDs for consistent matching\n",
    "user_ratings['imdbId'] = user_ratings['imdbId'].str.replace('tt', '').astype(str).str.zfill(7)\n",
    "movies_with_imdb['imdbId'] = movies_with_imdb['imdbId'].astype(str).str.zfill(7)\n",
    "\n",
    "# Merge your ratings with the MovieLens IDs\n",
    "test_ratings = user_ratings.merge(movies_with_imdb, on=\"imdbId\", how=\"inner\")\n",
    "print(f\"Found {len(test_ratings)} of your ratings in the MovieLens dataset\")\n",
    "\n",
    "# Calculate z-scores and add user mean/std for denormalization later\n",
    "test_ratings['user_mean'] = test_ratings['rating'].mean()\n",
    "test_ratings['user_std'] = test_ratings['rating'].std()\n",
    "test_ratings['z_score'] = (test_ratings['rating'] - test_ratings['user_mean']) / test_ratings['user_std']\n",
    "\n",
    "# Check if required variables exist\n",
    "required_vars = ['movie_idx_map', 'movie_idx_map', 'user_movie_csr', 'n_movies', 'n_users', 'faiss_index']\n",
    "for var in required_vars:\n",
    "    if var not in globals():\n",
    "        print(f\"Error: Variable '{var}' is not defined. Run the main ANN code first.\")\n",
    "        # Uncomment this to stop execution if variables are missing\n",
    "        # raise NameError(f\"Variable '{var}' is not defined\")\n",
    "\n",
    "# Generate predictions for your movies\n",
    "personal_predictions = []\n",
    "for _, row in test_ratings.iterrows():\n",
    "    movie_id = row['movieId']\n",
    "    if movie_id in movie_idx_map:\n",
    "        movie_idx = movie_idx_map[movie_id]\n",
    "        \n",
    "        # Get your normalized user vector (from your existing ratings)\n",
    "        user_vector = np.zeros((1, n_movies), dtype=np.float32)\n",
    "        \n",
    "        # Create a user vector from your ratings\n",
    "        for _, r in test_ratings.iterrows():\n",
    "            if r['movieId'] in movie_idx_map:\n",
    "                m_idx = movie_idx_map[r['movieId']]\n",
    "                user_vector[0, m_idx] = r['z_score']\n",
    "        \n",
    "        # Replace NaN values\n",
    "        user_vector = np.nan_to_num(user_vector, nan=0.0)\n",
    "        \n",
    "        # Find similar users excluding yourself (since you won't be in the training data)\n",
    "        D, I = faiss_index.search(user_vector, k=30)  # Use more neighbors for better predictions\n",
    "        \n",
    "        # Get ratings from similar users for this movie\n",
    "        similar_ratings = []\n",
    "        similar_distances = []\n",
    "        for i, neighbor_idx in enumerate(I[0]):\n",
    "            if 0 <= neighbor_idx < n_users:  # Valid index\n",
    "                rating = user_movie_csr[neighbor_idx, movie_idx]\n",
    "                if rating != 0:  # Rating exists\n",
    "                    similar_ratings.append(float(rating))\n",
    "                    similar_distances.append(D[0][i])\n",
    "        \n",
    "        if similar_ratings:\n",
    "            # Fix weights sum to zero issue\n",
    "            weights = np.exp(-np.array(similar_distances))\n",
    "            \n",
    "            # Check if weights sum to zero and fix if needed\n",
    "            if weights.sum() == 0:\n",
    "                # Use a simple average instead\n",
    "                weighted_avg = np.mean(similar_ratings)\n",
    "            else:\n",
    "                # Use weighted average\n",
    "                weighted_avg = np.average(similar_ratings, weights=weights)\n",
    "            \n",
    "            # Convert back to rating scale\n",
    "            predicted_rating = (weighted_avg * row['user_std']) + row['user_mean']\n",
    "            \n",
    "            # Clip predictions to valid rating range\n",
    "            predicted_rating = max(1, min(5, predicted_rating))\n",
    "            \n",
    "            personal_predictions.append({\n",
    "                'movieId': movie_id,\n",
    "                'title': row['title'],\n",
    "                'actual': row['rating'],\n",
    "                'predicted': predicted_rating,\n",
    "                'difference': row['rating'] - predicted_rating\n",
    "            })\n",
    "\n",
    "# Create DataFrame from predictions\n",
    "personal_df = pd.DataFrame(personal_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "if len(personal_df) > 0:\n",
    "    rmse = np.sqrt(mean_squared_error(personal_df['actual'], personal_df['predicted']))\n",
    "    mae = mean_absolute_error(personal_df['actual'], personal_df['predicted'])\n",
    "    \n",
    "    print(f\"\\nYour Personal Recommendation Results:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Number of predictions: {len(personal_df)}\")\n",
    "    \n",
    "    # Sort by largest discrepancies\n",
    "    personal_df['abs_diff'] = personal_df['difference'].abs()\n",
    "    personal_df = personal_df.sort_values('abs_diff', ascending=False)\n",
    "    \n",
    "    # Display the top matches and mismatches\n",
    "    print(\"\\nBiggest Rating Differences (Movies you might have unusual taste in):\")\n",
    "    print(personal_df.head(5)[['title', 'actual', 'predicted', 'difference']])\n",
    "    \n",
    "    print(\"\\nClosest Rating Matches (Movies the system understands your taste in):\")\n",
    "    print(personal_df.sort_values('abs_diff').head(5)[['title', 'actual', 'predicted', 'difference']])\n",
    "else:\n",
    "    print(\"No movies from your ratings were found in the dataset\")\n",
    "# The key addition I made is checking if the required variables exist before proceeding with the code. You need to run this code snippet after running the main ANN model code where these variables are defined.RetryClaude can make mistakes. Please double-check responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8b1136f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:46:16.178822Z",
     "start_time": "2025-03-16T17:46:13.765145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Movie Recommendations for You:\n",
      "39. Pulp Fiction - Recommendation Score: 1.33 (Recommended by 3 similar users)\n",
      "42. Alien - Recommendation Score: 1.12 (Recommended by 2 similar users)\n",
      "26. Terminator 2: Judgment Day - Recommendation Score: 1.10 (Recommended by 3 similar users)\n",
      "8. The Hunger Games: Catching Fire - Recommendation Score: 1.07 (Recommended by 2 similar users)\n",
      "31. Highlander - Recommendation Score: 0.98 (Recommended by 2 similar users)\n"
     ]
    }
   ],
   "source": [
    "def get_movie_recommendations(test_ratings, n_recommendations=5):\n",
    "    \"\"\"\n",
    "    Recommend new movies based on similar users' preferences\n",
    "    \n",
    "    Parameters:\n",
    "        test_ratings: DataFrame with your movie ratings\n",
    "        n_recommendations: Number of recommendations to return\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with recommended movies\n",
    "    \"\"\"\n",
    "    # Create your user vector from existing ratings\n",
    "    user_vector = np.zeros((1, n_movies), dtype=np.float32)\n",
    "    \n",
    "    # Fill in the user vector with your ratings (z-scores)\n",
    "    for _, row in test_ratings.iterrows():\n",
    "        if row['movieId'] in movie_idx_map:\n",
    "            movie_idx = movie_idx_map[row['movieId']]\n",
    "            user_vector[0, movie_idx] = row['z_score']\n",
    "    \n",
    "    # Replace NaN values\n",
    "    user_vector = np.nan_to_num(user_vector, nan=0.0)\n",
    "    \n",
    "    # Find similar users\n",
    "    D, I = faiss_index.search(user_vector, k=100)  # Use more neighbors for better recommendations\n",
    "    \n",
    "    # Get the movies you've already rated\n",
    "    rated_movie_ids = set(test_ratings['movieId'])\n",
    "    \n",
    "    # Collect candidate movies from similar users\n",
    "    candidate_movies = {}\n",
    "    \n",
    "    # Create a simpler method to get movies rated highly by similar users\n",
    "    for i, neighbor_idx in enumerate(I[0]):\n",
    "        if 0 <= neighbor_idx < n_users:  # Valid index\n",
    "            similarity = np.exp(-D[0][i])  # Convert distance to similarity\n",
    "            \n",
    "            # Get this user's original movie ratings\n",
    "            for movie_id, movie_idx in movie_idx_map.items():\n",
    "                if movie_id not in rated_movie_ids:  # Skip movies you've already rated\n",
    "                    rating_z = user_movie_csr[neighbor_idx, movie_idx]\n",
    "                    \n",
    "                    if rating_z > 0.5:  # Only consider movies the user rated positively\n",
    "                        if movie_id not in candidate_movies:\n",
    "                            candidate_movies[movie_id] = {'count': 0, 'score_sum': 0, 'similarity_sum': 0}\n",
    "                        \n",
    "                        candidate_movies[movie_id]['count'] += 1\n",
    "                        candidate_movies[movie_id]['score_sum'] += rating_z * similarity\n",
    "                        candidate_movies[movie_id]['similarity_sum'] += similarity\n",
    "    \n",
    "    # Calculate scores for each candidate movie\n",
    "    recommendations = []\n",
    "    for movie_id, data in candidate_movies.items():\n",
    "        if data['count'] >= 2:  # Require at least 3 similar users to recommend\n",
    "            # Calculate weighted average score\n",
    "            if data['similarity_sum'] > 0:\n",
    "                score = data['score_sum'] / data['similarity_sum']\n",
    "            else:\n",
    "                score = data['score_sum'] / data['count']\n",
    "            \n",
    "            # Get movie details\n",
    "            movie_title = \"Unknown\"\n",
    "            \n",
    "            # Try to get movie details from the movies dataset\n",
    "            movie_data = movies_with_imdb[movies_with_imdb['movieId'] == movie_id]\n",
    "            if not movie_data.empty:\n",
    "                movie_title = movie_data.iloc[0]['title']\n",
    "            \n",
    "            recommendations.append({\n",
    "                'movieId': movie_id,\n",
    "                'title': movie_title,\n",
    "                'score': score,\n",
    "                'recommender_count': data['count']\n",
    "            })\n",
    "    \n",
    "    # Sort by score and select top N\n",
    "    recommendations_df = pd.DataFrame(recommendations)\n",
    "    if not recommendations_df.empty:\n",
    "        recommendations_df = recommendations_df.sort_values('score', ascending=False).head(n_recommendations)\n",
    "        \n",
    "    return recommendations_df\n",
    "\n",
    "# Get 5 movie recommendations\n",
    "recommendations = get_movie_recommendations(test_ratings, n_recommendations=5)\n",
    "\n",
    "if not recommendations.empty:\n",
    "    print(\"Top 5 Movie Recommendations for You:\")\n",
    "    for i, row in recommendations.iterrows():\n",
    "        print(f\"{i+1}. {row['title']} - Recommendation Score: {row['score']:.2f} (Recommended by {row['recommender_count']} similar users)\")\n",
    "else:\n",
    "    print(\"Could not generate recommendations. Try increasing the number of similar users or reducing filtering criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2938111f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T18:17:50.332380Z",
     "start_time": "2025-03-16T18:17:47.790932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only these 3 movies for recommendations:\n",
      "                                                title  rating\n",
      "4                                                Sing     4.0\n",
      "58  Good, the Bad and the Ugly, The (Buono, il bru...     4.5\n",
      "0                                   Big Lebowski, The     3.5\n",
      "\n",
      "Recommendations based on only 3 movies:\n",
      "21. American Beauty - Score: 1.39 (Recommended by 3 similar users)\n",
      "43. Inside Man - Score: 1.16 (Recommended by 2 similar users)\n",
      "28. Terminator 2: Judgment Day - Score: 1.09 (Recommended by 3 similar users)\n",
      "24. Lord of the Rings: The Two Towers, The - Score: 1.08 (Recommended by 2 similar users)\n",
      "17. Harry Potter and the Prisoner of Azkaban - Score: 1.06 (Recommended by 2 similar users)\n"
     ]
    }
   ],
   "source": [
    "# Select just 3 random movies from your ratings\n",
    "import random\n",
    "minimal_ratings = test_ratings.sample(n=3)\n",
    "print(\"Using only these 3 movies for recommendations:\")\n",
    "print(minimal_ratings[['title', 'rating']])\n",
    "\n",
    "# Get recommendations based on just these 3 movies\n",
    "# Modify this to match your actual function definition\n",
    "minimal_recommendations = get_movie_recommendations(\n",
    "    minimal_ratings, \n",
    "    n_recommendations=5\n",
    ")\n",
    "\n",
    "print(\"\\nRecommendations based on only 3 movies:\")\n",
    "if not minimal_recommendations.empty:\n",
    "    for i, row in minimal_recommendations.iterrows():\n",
    "        print(f\"{i+1}. {row['title']} - Score: {row['score']:.2f} (Recommended by {row['recommender_count']} similar users)\")\n",
    "else:\n",
    "    print(\"Could not generate recommendations with just 3 movies. Try different movies or adjust parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7a3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "377.188px",
    "left": "1636.67px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
